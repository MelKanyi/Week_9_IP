{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moringa_Data_Science_Core_W9_Independent_Project_2020_Melissa_Kanyi_Python_notebook",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOdY/gvb4AWTJt5NhPSIHGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelKanyi/Week_9_IP/blob/master/Moringa_Data_Science_Core_W9_Independent_Project_2020_Melissa_Kanyi_Python_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRYBCFueBG9q",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"yellow\"> **Defining the Question**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdcewutKFRZH",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Specifying the Question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9MnQxqBNI7",
        "colab_type": "text"
      },
      "source": [
        "- Predict if a passenger survived the sinking of the Titanic or not.\n",
        "- Predict whether an email is spam or ham."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkaae9L0BaUV",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Metric of Success"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulcunipBZDs",
        "colab_type": "text"
      },
      "source": [
        "* Since both projects are classification problems, we will use:\n",
        "     * Accuracy; threshold 85%\n",
        "* For target class imbalance we will use \n",
        "* (harmonic mean between the positive rate (precision) and the negative rate (Recall))\n",
        "     * F1 score; threhold 85%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1EMMMqvBgCv",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Experimental Design Taken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVQ2FEYNBymo",
        "colab_type": "text"
      },
      "source": [
        " **Predicting survival in the titanic Disaster**\n",
        " * Loading the dataset\n",
        " * Exploratory Data Analysis\n",
        " * Visualization\n",
        " * Data Cleaning\n",
        " * Features Engineering\n",
        " * Modeling: K-Nearest Neighbors Classifier (KNN)\n",
        " * Hyperparameter Tuning\n",
        " * Optimization techinques for KNN \n",
        " * Recommendations\n",
        " * Challenging the model: Random Forest Classifier\n",
        " * Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hayAjVnUB_Mf",
        "colab_type": "text"
      },
      "source": [
        "## <font color=\"red\"> Appropriateness of the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XP0YwCi5CFZc",
        "colab_type": "text"
      },
      "source": [
        "**Dataset 1 links:**\n",
        "* Train set: [link text](https://www.kaggle.com/c/titanic/download/train.csv)\n",
        "* Test set:[link text](https://www.kaggle.com/c/titanic/download/test.csv)\n",
        "\n",
        "The dataset contains the following fields:\n",
        "* Pclass Ticket class (: 1=upper, 2=middle, 3=lower)\n",
        "* Sex : Gender\n",
        "* Age : Age in years (fractional for babies)\n",
        "* Sibsp : Number of siblings and spouse. Sibling = brother, sister, stepbrother, stepsister.    Spouse = husband, wife (mistresses and fiancés were ignored)\n",
        "* Parch: Number of parents or children aboard the ship. Parent = mother, father. Child = daughter, son, stepdaughter, stepson. Some children travelled only with a nanny, therefore parch=0 for them.\n",
        "* Ticket: Ticket number (a string of characters)\n",
        "* Fare: Passenger fare (dollars)\n",
        "* Cabin: Cabin number (a string of characters)\n",
        "* Embarked: Port of embarkation (S=Southampton, Q=Queenstown (now Cobh), C=Cherbourg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okdFgcgz3yzH",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"yellow\"> **Reading the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsOG5538otUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "# Importing visualization libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sklearn libraries for data preparartion and performance measures\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold, cross_val_predict\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, normalize, Normalizer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Algorithms\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YkhBrprqVOD",
        "colab_type": "code",
        "outputId": "7c56038d-db2a-4bb8-ced7-b2681c1ca2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        }
      },
      "source": [
        "# Loading the train dataset\n",
        "data = pd.read_csv('train.csv')\n",
        "data  "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Montvila, Rev. Juozas</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211536</td>\n",
              "      <td>13.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Graham, Miss. Margaret Edith</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>112053</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>B42</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>W./C. 6607</td>\n",
              "      <td>23.4500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Behr, Mr. Karl Howell</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111369</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>C148</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Dooley, Mr. Patrick</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>370376</td>\n",
              "      <td>7.7500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>891 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0              1         0       3  ...   7.2500   NaN         S\n",
              "1              2         1       1  ...  71.2833   C85         C\n",
              "2              3         1       3  ...   7.9250   NaN         S\n",
              "3              4         1       1  ...  53.1000  C123         S\n",
              "4              5         0       3  ...   8.0500   NaN         S\n",
              "..           ...       ...     ...  ...      ...   ...       ...\n",
              "886          887         0       2  ...  13.0000   NaN         S\n",
              "887          888         1       1  ...  30.0000   B42         S\n",
              "888          889         0       3  ...  23.4500   NaN         S\n",
              "889          890         1       1  ...  30.0000  C148         C\n",
              "890          891         0       3  ...   7.7500   NaN         Q\n",
              "\n",
              "[891 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNkQGKD746Ij",
        "colab_type": "code",
        "outputId": "292d515a-58e9-4722-9a90-8bde5a80fe3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "# Loading the test dataset\n",
        "test = pd.read_csv('test.csv')\n",
        "test"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>1305</td>\n",
              "      <td>3</td>\n",
              "      <td>Spector, Mr. Woolf</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A.5. 3236</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>1306</td>\n",
              "      <td>1</td>\n",
              "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
              "      <td>female</td>\n",
              "      <td>39.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17758</td>\n",
              "      <td>108.9000</td>\n",
              "      <td>C105</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>1307</td>\n",
              "      <td>3</td>\n",
              "      <td>Saether, Mr. Simon Sivertsen</td>\n",
              "      <td>male</td>\n",
              "      <td>38.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SOTON/O.Q. 3101262</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>1308</td>\n",
              "      <td>3</td>\n",
              "      <td>Ware, Mr. Frederick</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>359309</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>1309</td>\n",
              "      <td>3</td>\n",
              "      <td>Peter, Master. Michael J</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2668</td>\n",
              "      <td>22.3583</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>418 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId  Pclass  ... Cabin Embarked\n",
              "0            892       3  ...   NaN        Q\n",
              "1            893       3  ...   NaN        S\n",
              "2            894       2  ...   NaN        Q\n",
              "3            895       3  ...   NaN        S\n",
              "4            896       3  ...   NaN        S\n",
              "..           ...     ...  ...   ...      ...\n",
              "413         1305       3  ...   NaN        S\n",
              "414         1306       1  ...  C105        C\n",
              "415         1307       3  ...   NaN        S\n",
              "416         1308       3  ...   NaN        S\n",
              "417         1309       3  ...   NaN        C\n",
              "\n",
              "[418 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TMCheEn_ITn",
        "colab_type": "text"
      },
      "source": [
        "> We will not be using the test dataset because it does not have out dependent variable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVoj2g2HCyO0",
        "colab_type": "code",
        "outputId": "55327679-f38c-4ebc-98cb-e5a009416029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# checking the top of the data\n",
        "data.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  ...     Fare Cabin  Embarked\n",
              "0            1         0       3  ...   7.2500   NaN         S\n",
              "1            2         1       1  ...  71.2833   C85         C\n",
              "2            3         1       3  ...   7.9250   NaN         S\n",
              "3            4         1       1  ...  53.1000  C123         S\n",
              "4            5         0       3  ...   8.0500   NaN         S\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6YzbPSjC8go",
        "colab_type": "code",
        "outputId": "61cb7ed8-3efb-4191-994c-1b0a4bde777e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# checking the bottom of the data\n",
        "data.tail()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>887</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>Montvila, Rev. Juozas</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211536</td>\n",
              "      <td>13.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>888</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Graham, Miss. Margaret Edith</td>\n",
              "      <td>female</td>\n",
              "      <td>19.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>112053</td>\n",
              "      <td>30.00</td>\n",
              "      <td>B42</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>889</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
              "      <td>female</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>W./C. 6607</td>\n",
              "      <td>23.45</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>890</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Behr, Mr. Karl Howell</td>\n",
              "      <td>male</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>111369</td>\n",
              "      <td>30.00</td>\n",
              "      <td>C148</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>891</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Dooley, Mr. Patrick</td>\n",
              "      <td>male</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>370376</td>\n",
              "      <td>7.75</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     PassengerId  Survived  Pclass  ...   Fare Cabin  Embarked\n",
              "886          887         0       2  ...  13.00   NaN         S\n",
              "887          888         1       1  ...  30.00   B42         S\n",
              "888          889         0       3  ...  23.45   NaN         S\n",
              "889          890         1       1  ...  30.00  C148         C\n",
              "890          891         0       3  ...   7.75   NaN         Q\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3clk4UbDDDww",
        "colab_type": "text"
      },
      "source": [
        "From the table above: \n",
        "\n",
        "1. We need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them.\n",
        "\n",
        "2. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. \n",
        "\n",
        "3. We can also spot some more features, that contain missing values (NaN = not a number), that we need to deal with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72aSKjxPDN9e",
        "colab_type": "code",
        "outputId": "c160e7e5-5d8c-497e-f277-aaeef6512209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "# view summary information of our dataset\n",
        "data.info()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            "PassengerId    891 non-null int64\n",
            "Survived       891 non-null int64\n",
            "Pclass         891 non-null int64\n",
            "Name           891 non-null object\n",
            "Sex            891 non-null object\n",
            "Age            714 non-null float64\n",
            "SibSp          891 non-null int64\n",
            "Parch          891 non-null int64\n",
            "Ticket         891 non-null object\n",
            "Fare           891 non-null float64\n",
            "Cabin          204 non-null object\n",
            "Embarked       889 non-null object\n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "235g2hMkDj6H",
        "colab_type": "code",
        "outputId": "b34629df-5408-4d5a-acae-3902c3415cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "# checking for unique values \n",
        "\n",
        "cols = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n",
        "\n",
        "for col in cols:\n",
        "  print(col)\n",
        "  print(data[col].unique())\n",
        "  print('\\n')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pclass\n",
            "[3 1 2]\n",
            "\n",
            "\n",
            "Sex\n",
            "['male' 'female']\n",
            "\n",
            "\n",
            "SibSp\n",
            "[1 0 3 4 2 5 8]\n",
            "\n",
            "\n",
            "Parch\n",
            "[0 1 2 5 3 4 6]\n",
            "\n",
            "\n",
            "Embarked\n",
            "['S' 'C' 'Q' nan]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajGsYisXDok8",
        "colab_type": "code",
        "outputId": "b7f91a66-93a3-45ea-ea7c-8a756edda40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "\n",
        "# Statistical description of numerical columns\n",
        "data.describe()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>714.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "      <td>891.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.383838</td>\n",
              "      <td>2.308642</td>\n",
              "      <td>29.699118</td>\n",
              "      <td>0.523008</td>\n",
              "      <td>0.381594</td>\n",
              "      <td>32.204208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>257.353842</td>\n",
              "      <td>0.486592</td>\n",
              "      <td>0.836071</td>\n",
              "      <td>14.526497</td>\n",
              "      <td>1.102743</td>\n",
              "      <td>0.806057</td>\n",
              "      <td>49.693429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>223.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>20.125000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.910400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>446.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>668.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>31.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>891.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>512.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PassengerId    Survived      Pclass  ...       SibSp       Parch        Fare\n",
              "count   891.000000  891.000000  891.000000  ...  891.000000  891.000000  891.000000\n",
              "mean    446.000000    0.383838    2.308642  ...    0.523008    0.381594   32.204208\n",
              "std     257.353842    0.486592    0.836071  ...    1.102743    0.806057   49.693429\n",
              "min       1.000000    0.000000    1.000000  ...    0.000000    0.000000    0.000000\n",
              "25%     223.500000    0.000000    2.000000  ...    0.000000    0.000000    7.910400\n",
              "50%     446.000000    0.000000    3.000000  ...    0.000000    0.000000   14.454200\n",
              "75%     668.500000    1.000000    3.000000  ...    1.000000    0.000000   31.000000\n",
              "max     891.000000    1.000000    3.000000  ...    8.000000    6.000000  512.329200\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh4jlTbaD3JP",
        "colab_type": "text"
      },
      "source": [
        "Above we can see that 38% out of the dataset survived the Titanic.\n",
        "\n",
        "We can also see that the passenger ages range from 0.4 to 80.\n",
        "\n",
        "On top of that we can already detect some features, that contain missing values, like the ‘Age’ feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rTL6xk2Mr8Q",
        "colab_type": "text"
      },
      "source": [
        "* It is clear that people from the First class had higher chances of survival.\n",
        "* Pclass is therefore an important feature to predict survival"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiFor-STL0-9",
        "colab_type": "text"
      },
      "source": [
        "* Survival chance of women are higher between 14 and 40\n",
        "* Men have a high probability of survival when they are between 18 and 30 years old.\n",
        "* In both generally infants have a little bit of higher chances of survival.\n",
        "* Certain ages hava increased odds of survival.\n",
        "* Creating age groups in our feature engineering may help to make every feature of the same scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cy7hwGa_ahx",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"yellow\"> **Data Cleaning** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0FAJvufii8l",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Cleaning the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBtX8JuxVOj0",
        "colab_type": "code",
        "outputId": "d054627f-94a5-483c-fe09-b3b8eb7bd362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# checking data types\n",
        "data.dtypes"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      int64\n",
              "Survived         int64\n",
              "Pclass           int64\n",
              "Name            object\n",
              "Sex             object\n",
              "Age            float64\n",
              "SibSp            int64\n",
              "Parch            int64\n",
              "Ticket          object\n",
              "Fare           float64\n",
              "Cabin           object\n",
              "Embarked        object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9-5ZaHNVBeZ",
        "colab_type": "text"
      },
      "source": [
        "Which features are:\n",
        "\n",
        "1. **Categorical ?**\n",
        "- Survived\n",
        "- Sex\n",
        "- Embarked\n",
        "\n",
        "2. **Ordinal?**\n",
        "- Pclass.\n",
        "\n",
        "3. **Numerical?**\n",
        "- Age (Continuous)\n",
        "- Fare (Continuous)\n",
        "- SibSp (Discrete)\n",
        "- Parch (Discrete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYOhIR7BVWhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert the relevant data types to categorical\n",
        "\n",
        "# Creating a list of the numerical columns in the dataset.\n",
        "numeric = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "\n",
        "categoricals = ['Survived','Pclass','Sex','Embarked']\n",
        "\n",
        "objects = ['Name']\n",
        "\n",
        "for x in data.columns:\n",
        "   if x in numeric:\n",
        "       data[x]=pd.to_numeric(data[x])\n",
        "   elif x in categoricals:\n",
        "        data[x]=data[x].astype('category')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCwTfzQeVbop",
        "colab_type": "code",
        "outputId": "6b1f2ce7-fb03-475e-d77e-918ebafc908b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "#Check if features are assigned the relevant data types\n",
        "data.dtypes"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId       int64\n",
              "Survived       category\n",
              "Pclass         category\n",
              "Name             object\n",
              "Sex            category\n",
              "Age             float64\n",
              "SibSp             int64\n",
              "Parch             int64\n",
              "Ticket           object\n",
              "Fare            float64\n",
              "Cabin            object\n",
              "Embarked       category\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voN31qLb8xzC",
        "colab_type": "code",
        "outputId": "1966f181-5924-4f88-b627-00fa30308559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# checking for missing values \n",
        "data.isnull().any()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId    False\n",
              "Survived       False\n",
              "Pclass         False\n",
              "Name           False\n",
              "Sex            False\n",
              "Age             True\n",
              "SibSp          False\n",
              "Parch          False\n",
              "Ticket         False\n",
              "Fare           False\n",
              "Cabin           True\n",
              "Embarked        True\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3fQ-OSD_ukt",
        "colab_type": "code",
        "outputId": "7b807bfe-94ec-4e92-d454-12c57ec97892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age            177\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcu-sAuerxf",
        "colab_type": "text"
      },
      "source": [
        "The Embarked feature has only 2 missing values, which can easily be filled by mode.\n",
        "\n",
        "It will be much more tricky, to deal with the ‘Age’ feature, which has 177 missing values.\n",
        "\n",
        "The ‘Cabin’ feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxT63ivxesmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replacing all missing values with the mean in the age column\n",
        "data['Age'].fillna((data['Age'].mean()), inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_N0Cx4QPsEM",
        "colab_type": "code",
        "outputId": "7f3df475-ba85-4181-8cd2-711d5afa9078",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# confirming changes\n",
        "data.isnull().sum()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age              0\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         2\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUHXekFUWShD",
        "colab_type": "text"
      },
      "source": [
        "The Embarked column has 2 missing values. It's best to fill them with the most common values in that column to maintain its frequency distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faH6_gdXf4y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replace the null values in the Embarked column with the mode\n",
        "\n",
        "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBJ_Fs0iP2vP",
        "colab_type": "code",
        "outputId": "19da706c-3a4b-4eb9-de31-7f8da8211eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "# confirming changes\n",
        "data.isnull().sum()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Survived         0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age              0\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             0\n",
              "Cabin          687\n",
              "Embarked         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9V75CmDQRNc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# column cabin containing categorical values \n",
        "# dropping the columns since it contains very many missing values\n",
        "data.drop('Cabin', axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY2Ve4q3QeWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropping the unnecessary columns\n",
        "data.drop(['PassengerId', 'Ticket'], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zortzdU9QUFp",
        "colab_type": "code",
        "outputId": "fdbc54f1-d2dc-41dd-fc86-00e4d892bdb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# confirming changes\n",
        "data.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Survived Pclass  ...     Fare Embarked\n",
              "0        0      3  ...   7.2500        S\n",
              "1        1      1  ...  71.2833        C\n",
              "2        1      3  ...   7.9250        S\n",
              "3        1      1  ...  53.1000        S\n",
              "4        0      3  ...   8.0500        S\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMuH9WAQg6Wt",
        "colab_type": "code",
        "outputId": "6c87a13e-9bf4-48c9-ad64-63860033a8aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# checking if the data set contains duplicates\n",
        "data.duplicated(subset = None, keep = 'first')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      False\n",
              "1      False\n",
              "2      False\n",
              "3      False\n",
              "4      False\n",
              "       ...  \n",
              "886    False\n",
              "887    False\n",
              "888    False\n",
              "889    False\n",
              "890    False\n",
              "Length: 891, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWkpfXRoQ0aN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# droping the duplicated values \n",
        "data.drop_duplicates(subset = None, keep = 'first', inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxzkGqbnRFu0",
        "colab_type": "code",
        "outputId": "2d8d45ab-9ebb-4ae3-a169-d8dbf894abb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# checking if the data set contains duplicates\n",
        "data.duplicated(subset = None, keep = 'first')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      False\n",
              "1      False\n",
              "2      False\n",
              "3      False\n",
              "4      False\n",
              "       ...  \n",
              "886    False\n",
              "887    False\n",
              "888    False\n",
              "889    False\n",
              "890    False\n",
              "Length: 891, dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0EkLR6ALJGN",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"yellow\"> **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXL6WDPeXYEO",
        "colab_type": "text"
      },
      "source": [
        "Plot a Correlation Matrix to establish the significance among features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWK3jjtSXSB3",
        "colab_type": "code",
        "outputId": "b762b63e-a0ed-41f5-a046-2f3e6b981b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "corr = data.corr()\n",
        "\n",
        "ax = sns.heatmap(corr,square=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAEvCAYAAAB2cWuyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEnNERMMkMUdENEwSc0REw/x/DCQLW5gLs1YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ-G0nYyXhh-",
        "colab_type": "code",
        "outputId": "bdef007b-5c18-4c32-9b64-cdfd0db473e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "data.Survived.value_counts()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    549\n",
              "1    342\n",
              "Name: Survived, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUEWv2Ckvokz",
        "colab_type": "code",
        "outputId": "b2c65350-d1bb-4001-c003-1c275ff35a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# Using a bar chart from seaborn\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "sns.barplot(x='Pclass', y='Survived', data=data)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7faa4d393198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEMCAYAAAAxoErWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKAkAgCFKAgBg6P8A54DrXrneCQYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TG2ODb9QGA9",
        "colab_type": "code",
        "outputId": "e96319c3-ee31-4b3c-ddbb-763150db02de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "\n",
        "sex = pd.DataFrame({'count': data.groupby(['Pclass','Sex']).size()}).reset_index()\n",
        "# sns.barplot(x = 'Pclass', y = 'count', hue = 'Sex', data = sex)\n",
        "g = sns.catplot(x = 'Sex', y = 'count',\n",
        "           col = 'Pclass', data = sex,\n",
        "           kind = 'bar', aspect = 0.5, height = 6)\n",
        "g.set_axis_labels('Gender', 'Number of passengers')\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAGkCAYAAACraPiJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 648x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LC9aqS5LJ2M",
        "colab_type": "code",
        "outputId": "d86b1e62-04a5-4f9c-fb37-8d9e98debd1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "# Using Histograms to compare age and sex and their contribution to survival of a person\n",
        "# Plot using seaborn\n",
        "sns.set(style = 'whitegrid', context = 'notebook')\n",
        "# First we create variables labeling the survived column values 0 and 1\n",
        "survived = 'survived'\n",
        "not_survived = 'not survived'\n",
        "\n",
        "\n",
        "# creating a chart where our plots will appear\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\n",
        "\n",
        "# creating women and male variables from the male and female variables in the dataset\n",
        "women = data[data['Sex']=='female']\n",
        "men = data[data['Sex']=='male']\n",
        "\n",
        "# Plotting the histogram of the women and specifying the bin sizes, and labels as we created earlier\n",
        "\n",
        "ax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\n",
        "ax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\n",
        "ax.legend()\n",
        "ax.set_title('Female')\n",
        "\n",
        "# Plotting the histogram of the men and specifying the bin sizes, and labels as we created earlier\n",
        "ax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\n",
        "ax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\n",
        "ax.legend()\n",
        "_ = ax.set_title('Male')\n",
        "plt.show()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAEcCAYAAADjpzHMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTBERERHJ8P+Ub5fAht3kSQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzcLfIBVWK5v",
        "colab_type": "text"
      },
      "source": [
        "# <font color = \"Yellow\"> **Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgpCB-alY60R",
        "colab_type": "text"
      },
      "source": [
        "As was stated earlier women, children and the upper class were given priority to get to safety, we therefore have to establish the title of the passengers, the size of a family or whether the passenger was onboard alone.\n",
        "\n",
        "It is difficult to establish any information on a passenger based on their names, some are nicknames. So we try establish the title of each passenger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6U6kKxWY6ZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's establish the title of each passenger\n",
        "\n",
        "for x in data:\n",
        "    data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYvW1kH4ZJb9",
        "colab_type": "code",
        "outputId": "3264f3a2-5885-44dd-908c-7ecd71579626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#What are the titles extracted from the names ?\n",
        "\n",
        "data.Title.unique()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Mr', 'Mrs', 'Miss', 'Master', 'Don', 'Rev', 'Dr', 'Mme', 'Ms',\n",
              "       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess',\n",
              "       'Jonkheer'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D07jdJQ6ZOAK",
        "colab_type": "text"
      },
      "source": [
        "The titles are many in number and usually we just need to establish if it's a Miss, Mrs, Mr or Master and every other title can be referred to as 'other'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQuJkMQ3ZK3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in data:\n",
        "    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')\n",
        "\n",
        "    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n",
        "    data['Title'] = data['Title'].replace('Ms', 'Miss')\n",
        "    data['Title'] = data['Title'].replace('Mme', 'Mrs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-VqgBiOZejR",
        "colab_type": "code",
        "outputId": "2f3505e6-1bd4-45db-c7cf-bb876f31e775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#What are the titles extracted from the names?\n",
        "\n",
        "data.Title.unique()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rqMXleUZpZe",
        "colab_type": "text"
      },
      "source": [
        "Now we can safely drop the Name column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8F-BOIdZiyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropping the name column\n",
        "data.drop('Name', axis=1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgyNVuORZ78w",
        "colab_type": "code",
        "outputId": "7e141424-dca9-4634-9495-a0518afb64ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# confirming changes\n",
        "data.head(2)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>Mr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>Mrs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Survived Pclass     Sex   Age  SibSp  Parch     Fare Embarked Title\n",
              "0        0      3    male  22.0      1      0   7.2500        S    Mr\n",
              "1        1      1  female  38.0      1      0  71.2833        C   Mrs"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LavkjORoaR_S",
        "colab_type": "text"
      },
      "source": [
        "We can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qINyEgUmaIAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in data:\n",
        "    data['FamSize'] = data['SibSp'] + data['Parch'] + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLrBB3lTaYv-",
        "colab_type": "code",
        "outputId": "6393cb5c-64b2-414c-c849-3eadc2481108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# confirming changes\n",
        "data.head(2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Title</th>\n",
              "      <th>FamSize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>S</td>\n",
              "      <td>Mr</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C</td>\n",
              "      <td>Mrs</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Survived Pclass     Sex   Age  SibSp  Parch     Fare Embarked Title  FamSize\n",
              "0        0      3    male  22.0      1      0   7.2500        S    Mr        2\n",
              "1        1      1  female  38.0      1      0  71.2833        C   Mrs        2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-gzzFlvak95",
        "colab_type": "text"
      },
      "source": [
        "Was the passenger traveling alone or with a family? If the family size is 1 then they are probably alone, if there are more then the passenger is essentially not alone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6XzK5xtakLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in data:\n",
        "    data['Alone?'] = 0\n",
        "    data.loc[data['FamSize'] == 1, 'Alone?'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJK1g_CWasMM",
        "colab_type": "code",
        "outputId": "62addef1-c45a-4ddc-abf8-ff17736719be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data['Alone?'].unique()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bNYwYOCsify",
        "colab_type": "text"
      },
      "source": [
        "Essentially;\n",
        "\n",
        "if Alone = 0, they are not alone\n",
        "\n",
        "if Alone = 1, they are alone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwzG5o4gvCcs",
        "colab_type": "code",
        "outputId": "7e3aca8a-fcb9-4fb3-e1e5-e6178ea953ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "data.head(1)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Title</th>\n",
              "      <th>FamSize</th>\n",
              "      <th>Alone?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.25</td>\n",
              "      <td>S</td>\n",
              "      <td>Mr</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Survived Pclass   Sex   Age  SibSp  ...  Fare  Embarked Title FamSize  Alone?\n",
              "0        0      3  male  22.0      1  ...  7.25         S    Mr       2       0\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuTueVdCsres",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Thereafter we can drop the colums 'SibSp','Parch' and 'FamSize'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLWjvCg5ssZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop(['Parch', 'SibSp', 'FamSize'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbj7mCwcstq3",
        "colab_type": "code",
        "outputId": "d85e881e-e9a5-4478-c89c-379fd55b134b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        }
      },
      "source": [
        "data.head(1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Title</th>\n",
              "      <th>Alone?</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>7.25</td>\n",
              "      <td>S</td>\n",
              "      <td>Mr</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Survived Pclass   Sex   Age  Fare Embarked Title  Alone?\n",
              "0        0      3  male  22.0  7.25        S    Mr       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQdpjqyktAAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categoricals = ['Sex','Embarked','Title']\n",
        "\n",
        "data = pd.get_dummies(data, columns=['Sex','Embarked','Title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXIWXMwhtIoA",
        "colab_type": "code",
        "outputId": "19bc297e-9026-4582-fdef-d3eff563f740",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "data.head(2)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Alone?</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Title_Master</th>\n",
              "      <th>Title_Miss</th>\n",
              "      <th>Title_Mr</th>\n",
              "      <th>Title_Mrs</th>\n",
              "      <th>Title_Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>22.0</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38.0</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Survived Pclass   Age     Fare  ...  Title_Miss  Title_Mr  Title_Mrs  Title_Other\n",
              "0        0      3  22.0   7.2500  ...           0         1          0            0\n",
              "1        1      1  38.0  71.2833  ...           0         0          1            0\n",
              "\n",
              "[2 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP9yQc2SbiFy",
        "colab_type": "text"
      },
      "source": [
        "We must consider the age column, if a passenger was a child they had a higher chance of survival, therefore we must categorize a passenger as adult(1) or child(0) in a new Adult column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9YmvFJDvTg2",
        "colab_type": "code",
        "outputId": "25fdc743-8448-4cd0-cfb1-a23d86051c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "data[\"Adult\"] = 0\n",
        "\n",
        "data[\"Adult\"][data[\"Age\"] >= 18] = 1\n",
        "\n",
        "#Thereafter drop the age column\n",
        "\n",
        "data = data.drop(['Age'], axis=1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8P005JWvZgI",
        "colab_type": "code",
        "outputId": "7391863f-3342-44e0-c2a6-e59d49932ba3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "data = data[['Pclass','Adult','Fare','Alone?','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S','Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Other','Survived']]\n",
        "data.head()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Adult</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Alone?</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Title_Master</th>\n",
              "      <th>Title_Miss</th>\n",
              "      <th>Title_Mr</th>\n",
              "      <th>Title_Mrs</th>\n",
              "      <th>Title_Other</th>\n",
              "      <th>Survived</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8.0500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Pclass  Adult     Fare  Alone?  ...  Title_Mr  Title_Mrs  Title_Other  Survived\n",
              "0      3      1   7.2500       0  ...         1          0            0         0\n",
              "1      1      1  71.2833       0  ...         0          1            0         1\n",
              "2      3      1   7.9250       1  ...         0          0            0         1\n",
              "3      1      1  53.1000       0  ...         0          1            0         1\n",
              "4      3      1   8.0500       1  ...         1          0            0         0\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUclcZtf1qJz",
        "colab_type": "text"
      },
      "source": [
        "# <font color = yellow> **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNE-_dNJcHSv",
        "colab_type": "text"
      },
      "source": [
        "## <font color = \"red\"> K-Nearest Neighbors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyKKNHTYb6d4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split into train and test sets\n",
        "# define our X and y\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WygaX7bpcfJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train,y_test = train_test_split (X,y,test_size = 0.2,random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGfDpSdvclcF",
        "colab_type": "code",
        "outputId": "b61c2e7c-8a97-47f4-f386-b1de0f2e748b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"Shape of the X_train: \", X_train.shape)\n",
        "print(\"Shape of the y_train: \", y_train.shape)\n",
        "print(\"Shape of the X_test: \", X_test.shape)\n",
        "print(\"Shape of the y_test: \", y_test.shape)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the X_train:  (712, 14)\n",
            "Shape of the y_train:  (712,)\n",
            "Shape of the X_test:  (179, 14)\n",
            "Shape of the y_test:  (179,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF17S8xfc974",
        "colab_type": "code",
        "outputId": "89fb0a91-89af-4960-bc5e-5dc2ff3b5a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#Perform the K-Nearest Neighbors Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy score\n",
        "\n",
        "print('The accuracy :',accuracy_score(y_pred,y_test))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       105\n",
            "           1       0.75      0.73      0.74        74\n",
            "\n",
            "    accuracy                           0.79       179\n",
            "   macro avg       0.78      0.78      0.78       179\n",
            "weighted avg       0.79      0.79      0.79       179\n",
            "\n",
            "[[87 18]\n",
            " [20 54]]\n",
            "The accuracy : 0.7877094972067039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtK9-FkTdhxK",
        "colab_type": "text"
      },
      "source": [
        "Let us repeat the step using 70% training data and 30% test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnY4k6nudMAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train and test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test,y_train,y_test = train_test_split (X,y,test_size = 0.3,random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJKkf0D-dn-3",
        "colab_type": "code",
        "outputId": "19c5289e-e506-40d7-9f07-8965f6e017ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"Shape of the X_train: \", X_train.shape)\n",
        "print(\"Shape of the y_train: \", y_train.shape)\n",
        "print(\"Shape of the X_test: \", X_test.shape)\n",
        "print(\"Shape of the y_test: \", y_test.shape)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the X_train:  (623, 14)\n",
            "Shape of the y_train:  (623,)\n",
            "Shape of the X_test:  (268, 14)\n",
            "Shape of the y_test:  (268,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep2A9cH_dqYl",
        "colab_type": "code",
        "outputId": "405e2c5f-bac7-47fe-bf97-4e2f6a504706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#Perform the K-Nearest Neighbors Classifier\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy score\n",
        "\n",
        "print('The accuracy :',accuracy_score(y_pred,y_test))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83       157\n",
            "           1       0.77      0.73      0.75       111\n",
            "\n",
            "    accuracy                           0.80       268\n",
            "   macro avg       0.79      0.79      0.79       268\n",
            "weighted avg       0.80      0.80      0.80       268\n",
            "\n",
            "[[133  24]\n",
            " [ 30  81]]\n",
            "The accuracy : 0.7985074626865671\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVqo7CJOd3Yn",
        "colab_type": "text"
      },
      "source": [
        "Let's use 60% training data and 40% test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82OkOqhOdy3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train and test splits\n",
        "X_train, X_test,y_train,y_test = train_test_split (X,y,test_size = 0.4,random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJSR-n0geClp",
        "colab_type": "code",
        "outputId": "b956583d-5d6f-45dc-c514-d4fb8024e77f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "print(\"Shape of the X_train: \", X_train.shape)\n",
        "print(\"Shape of the y_train: \", y_train.shape)\n",
        "print(\"Shape of the X_test: \", X_test.shape)\n",
        "print(\"Shape of the y_test: \", y_test.shape)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of the X_train:  (534, 14)\n",
            "Shape of the y_train:  (534,)\n",
            "Shape of the X_test:  (357, 14)\n",
            "Shape of the y_test:  (357,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l180JW2deFAn",
        "colab_type": "code",
        "outputId": "4f6e44d0-c66c-46d6-d6a0-39b364779786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "#Perform the K-Nearest Neighbors Classifier\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Summary of the predictions made by the classifier\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Accuracy score\n",
        "\n",
        "print('The accuracy :',accuracy_score(y_pred,y_test))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       216\n",
            "           1       0.76      0.69      0.72       141\n",
            "\n",
            "    accuracy                           0.79       357\n",
            "   macro avg       0.78      0.77      0.78       357\n",
            "weighted avg       0.79      0.79      0.79       357\n",
            "\n",
            "[[185  31]\n",
            " [ 44  97]]\n",
            "The accuracy : 0.7899159663865546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2Y2f9LzvjxK",
        "colab_type": "text"
      },
      "source": [
        "### <font color = cyan> Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeZv2kyFv3rP",
        "colab_type": "text"
      },
      "source": [
        "Parameters to tune in KNN\n",
        "\n",
        "* k's\n",
        "* Distance Metric\n",
        "* Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBtzSMRceaWX",
        "colab_type": "text"
      },
      "source": [
        "### <font color = cyan> Optimize the K-Nearest Neighbors Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClYamf0jvwVy",
        "colab_type": "text"
      },
      "source": [
        "we can optimize KNN by:\n",
        "* Dimensionality Reduction with Linear Discriminant Analysis\n",
        "* Rescaling our data which makes the distance metric more meaningful. \n",
        "* Changing the distance **metric** for different applications.\n",
        "* Implementing weighted voting\n",
        "* Applying appropriate nearest-neighbor techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j9ApPT35KqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "21206514-11c8-449b-f4e3-5e27d3fe4267"
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:,-1]\n",
        "\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)\n",
        "\n",
        "# scaling our Data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# Reducing the dimensions in our dataset\n",
        "# Specifying the number of components\n",
        "lda = LDA(n_components=7)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "\n",
        "# Searching the best parameters using Rnadom Search\n",
        "params = { 'n_neighbors' : range(1,10),\n",
        "          'metric' : ['euclidean', 'manhattan', 'minkowski'],\n",
        "          'weights': ['uniform', 'distance']}\n",
        "          \n",
        "kfold = KFold(n_splits = 5, random_state = 10)  # specifying number of folds for cross validation\n",
        "        \n",
        "rs = RandomizedSearchCV(KNeighborsClassifier(),\n",
        "                       params,  cv = kfold,\n",
        "                       n_jobs = -1)\n",
        "          \n",
        "rs = rs.fit(X_train, y_train)\n",
        "rs.best_params_"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(14, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'metric': 'euclidean', 'n_neighbors': 8, 'weights': 'distance'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qJWXpqt53RD",
        "colab_type": "text"
      },
      "source": [
        "Using the Best parameters to assess the best split KNN model (80,20)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hxp8HX7-54CA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d7e3184a-707a-4472-fed9-d3a2ba0e6f83"
      },
      "source": [
        "# Splitting the data into features and target variable\n",
        "X = data.iloc[:, :-1]\n",
        "y = data.iloc[:,-1]\n",
        "\n",
        "# Splitting the data into test and train sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 10)\n",
        "\n",
        "\n",
        "# scaling our Data\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# Reducing the dimensions in our dataset\n",
        "# Specifying the number of components\n",
        "lda = LDA(n_components=7)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "# Instantiating our model\n",
        "# Using our best parameters using random search\n",
        "knc = KNeighborsClassifier(metric = 'euclidean', n_neighbors= 9, weights = 'uniform')\n",
        "\n",
        "# Fitting the KNN classifier to our train data set\n",
        "model2 = knc.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Making prediction the created model\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8212290502793296\n",
            "\n",
            "\n",
            "[[101  16]\n",
            " [ 16  46]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86       117\n",
            "           1       0.74      0.74      0.74        62\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.80      0.80      0.80       179\n",
            "weighted avg       0.82      0.82      0.82       179\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(14, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZkZ48Cr6GSM",
        "colab_type": "text"
      },
      "source": [
        "* There is great improvement in our tuned model.\n",
        "* The wrongly classified classes actually reduced significantly.\n",
        "* Interprating the confusion matrix:\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 104 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 13 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 17 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 45 were correctly classified as survived (true positives).\n",
        "     \n",
        "* The F1 score = 87%, Recall= 89% & Precision = 86% also improved greatly\n",
        "* Our model predicts 86% of the time, a passengers survival correctly (precision).\n",
        "* The recall tells us that it predicted the survival of 89% of the people who actually survived."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfRzT0blenDw",
        "colab_type": "text"
      },
      "source": [
        "Predict the scores using KNearestNeighbors (KNN) with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pd6qxR13eGTS",
        "colab_type": "code",
        "outputId": "1d583d19-ae56-4b13-e70f-2dba3130f219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "model = KNeighborsClassifier()\n",
        "k_range = list(range(1,10))\n",
        "weights_options = ['uniform','distance']\n",
        "k_grid = dict(n_neighbors=k_range, weights = weights_options)\n",
        "grid = GridSearchCV(model, k_grid, cv=10, scoring = 'precision')\n",
        "grid.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
              "                                            metric='minkowski',\n",
              "                                            metric_params=None, n_jobs=None,\n",
              "                                            n_neighbors=5, p=2,\n",
              "                                            weights='uniform'),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
              "                         'weights': ['uniform', 'distance']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='precision', verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 577
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68kRC6LBerpk",
        "colab_type": "code",
        "outputId": "932da8d0-b5a2-46a9-968d-87287826a928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "grid.cv_results_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'mean_fit_time': array([0.00490344, 0.00392129, 0.00389423, 0.0039592 , 0.00380168,\n",
              "        0.00394795, 0.00405011, 0.00383685, 0.0038553 , 0.00369191,\n",
              "        0.00385966, 0.00413589, 0.00443096, 0.00384734, 0.0039923 ,\n",
              "        0.00382383, 0.00439386, 0.00438299]),\n",
              " 'mean_score_time': array([0.00531802, 0.00339224, 0.00494053, 0.0034229 , 0.00479765,\n",
              "        0.00339794, 0.00535688, 0.00331664, 0.00495872, 0.00318084,\n",
              "        0.00494077, 0.00345004, 0.00550382, 0.00328739, 0.00495901,\n",
              "        0.00324695, 0.00564256, 0.00375152]),\n",
              " 'mean_test_score': array([0.65216554, 0.65216554, 0.77953439, 0.72162013, 0.72393717,\n",
              "        0.71634173, 0.73675824, 0.71251534, 0.71175813, 0.70841767,\n",
              "        0.73478167, 0.716566  , 0.67244579, 0.72286282, 0.69593048,\n",
              "        0.70293054, 0.63389086, 0.67475758]),\n",
              " 'param_n_neighbors': masked_array(data=[1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'param_weights': masked_array(data=['uniform', 'distance', 'uniform', 'distance',\n",
              "                    'uniform', 'distance', 'uniform', 'distance',\n",
              "                    'uniform', 'distance', 'uniform', 'distance',\n",
              "                    'uniform', 'distance', 'uniform', 'distance',\n",
              "                    'uniform', 'distance'],\n",
              "              mask=[False, False, False, False, False, False, False, False,\n",
              "                    False, False, False, False, False, False, False, False,\n",
              "                    False, False],\n",
              "        fill_value='?',\n",
              "             dtype=object),\n",
              " 'params': [{'n_neighbors': 1, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 1, 'weights': 'distance'},\n",
              "  {'n_neighbors': 2, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 2, 'weights': 'distance'},\n",
              "  {'n_neighbors': 3, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 3, 'weights': 'distance'},\n",
              "  {'n_neighbors': 4, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 4, 'weights': 'distance'},\n",
              "  {'n_neighbors': 5, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 5, 'weights': 'distance'},\n",
              "  {'n_neighbors': 6, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 6, 'weights': 'distance'},\n",
              "  {'n_neighbors': 7, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 7, 'weights': 'distance'},\n",
              "  {'n_neighbors': 8, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 8, 'weights': 'distance'},\n",
              "  {'n_neighbors': 9, 'weights': 'uniform'},\n",
              "  {'n_neighbors': 9, 'weights': 'distance'}],\n",
              " 'rank_test_score': array([16, 16,  1,  6,  4,  8,  2,  9, 10, 11,  3,  7, 15,  5, 13, 12, 18,\n",
              "        14], dtype=int32),\n",
              " 'split0_test_score': array([0.6       , 0.6       , 0.65      , 0.65217391, 0.68181818,\n",
              "        0.66666667, 0.75      , 0.65217391, 0.65217391, 0.65217391,\n",
              "        0.68421053, 0.65217391, 0.66666667, 0.68181818, 0.7       ,\n",
              "        0.60869565, 0.66666667, 0.63636364]),\n",
              " 'split1_test_score': array([0.56521739, 0.56521739, 0.66666667, 0.63157895, 0.68181818,\n",
              "        0.69565217, 0.625     , 0.66666667, 0.65217391, 0.66666667,\n",
              "        0.66666667, 0.65217391, 0.64      , 0.66666667, 0.68421053,\n",
              "        0.68181818, 0.69565217, 0.69565217]),\n",
              " 'split2_test_score': array([0.57894737, 0.57894737, 0.61538462, 0.64705882, 0.6875    ,\n",
              "        0.64705882, 0.8       , 0.64705882, 0.73684211, 0.70588235,\n",
              "        0.78571429, 0.6875    , 0.75      , 0.71428571, 0.76923077,\n",
              "        0.70588235, 0.66666667, 0.66666667]),\n",
              " 'split3_test_score': array([0.66666667, 0.66666667, 0.64705882, 0.68181818, 0.66666667,\n",
              "        0.69565217, 0.57142857, 0.72727273, 0.7       , 0.7       ,\n",
              "        0.68421053, 0.71428571, 0.66666667, 0.71428571, 0.66666667,\n",
              "        0.71428571, 0.61904762, 0.68181818]),\n",
              " 'split4_test_score': array([0.64705882, 0.64705882, 0.85714286, 0.64285714, 0.66666667,\n",
              "        0.64285714, 0.66666667, 0.61538462, 0.5625    , 0.5625    ,\n",
              "        0.58333333, 0.5625    , 0.5       , 0.58823529, 0.53846154,\n",
              "        0.52631579, 0.5       , 0.55555556]),\n",
              " 'split5_test_score': array([0.75      , 0.75      , 1.        , 0.78571429, 0.70588235,\n",
              "        0.75      , 0.83333333, 0.75      , 0.66666667, 0.6875    ,\n",
              "        0.76923077, 0.78571429, 0.66666667, 0.73333333, 0.75      ,\n",
              "        0.76923077, 0.5625    , 0.66666667]),\n",
              " 'split6_test_score': array([0.65      , 0.65      , 0.75      , 0.75      , 0.75      ,\n",
              "        0.6875    , 0.73333333, 0.70588235, 0.76470588, 0.70588235,\n",
              "        0.69230769, 0.70588235, 0.63157895, 0.6875    , 0.53333333,\n",
              "        0.61111111, 0.5625    , 0.5625    ]),\n",
              " 'split7_test_score': array([0.57692308, 0.57692308, 0.7       , 0.625     , 0.65      ,\n",
              "        0.63636364, 0.625     , 0.61904762, 0.57894737, 0.6       ,\n",
              "        0.625     , 0.6       , 0.57894737, 0.63157895, 0.58823529,\n",
              "        0.6       , 0.57894737, 0.6       ]),\n",
              " 'split8_test_score': array([0.73684211, 0.73684211, 1.        , 0.86666667, 0.86666667,\n",
              "        0.86666667, 0.91666667, 0.86666667, 0.92857143, 0.92857143,\n",
              "        1.        , 0.92307692, 0.84615385, 0.92857143, 0.91666667,\n",
              "        0.92307692, 0.76470588, 0.8       ]),\n",
              " 'split9_test_score': array([0.75      , 0.75      , 0.90909091, 0.93333333, 0.88235294,\n",
              "        0.875     , 0.84615385, 0.875     , 0.875     , 0.875     ,\n",
              "        0.85714286, 0.88235294, 0.77777778, 0.88235294, 0.8125    ,\n",
              "        0.88888889, 0.72222222, 0.88235294]),\n",
              " 'std_fit_time': array([0.00167651, 0.00021232, 0.00028145, 0.000266  , 0.00017505,\n",
              "        0.0002214 , 0.00023814, 0.00016735, 0.00030805, 0.00011662,\n",
              "        0.00017592, 0.00099497, 0.00076397, 0.00026195, 0.00032232,\n",
              "        0.00017879, 0.0010016 , 0.00034232]),\n",
              " 'std_score_time': array([0.00084847, 0.00019323, 0.00022055, 0.00043087, 0.00030219,\n",
              "        0.00021899, 0.00130133, 0.00035118, 0.00044903, 0.00023646,\n",
              "        0.00024404, 0.00045415, 0.0009405 , 0.0001495 , 0.0003316 ,\n",
              "        0.00011501, 0.00121987, 0.00026247]),\n",
              " 'std_test_score': array([0.0691794 , 0.0691794 , 0.14185533, 0.10314421, 0.07960824,\n",
              "        0.08345962, 0.1072727 , 0.08945899, 0.11237318, 0.10709411,\n",
              "        0.11646505, 0.11022449, 0.0941204 , 0.10027981, 0.11595353,\n",
              "        0.12129408, 0.07889052, 0.09657771])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 578
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00bAJXIgev5l",
        "colab_type": "code",
        "outputId": "2a9b2552-436b-41ce-d875-4cf46e844b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (\"Best Score: \",str(grid.best_score_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score:  0.779534387181446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4_X0FPEey59",
        "colab_type": "code",
        "outputId": "fc841372-7d5f-42ab-9bda-bef7523daa53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (\"Best Parameters: \",str(grid.best_params_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Parameters:  {'n_neighbors': 2, 'weights': 'uniform'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn6pfd6me3L8",
        "colab_type": "code",
        "outputId": "1f289674-0c8d-4f7c-efbb-d6bfe5569e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print (\"Best Estimators: \",str(grid.best_estimator_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Estimators:  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
            "                     weights='uniform')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-6ds1b3e5M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Predict scores\n",
        "\n",
        "y_pred = grid.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOdrqNVXe8hc",
        "colab_type": "code",
        "outputId": "3ae2f781-0013-4c4b-ab48-ea13e83fb37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Calculate Accuracy\n",
        "\n",
        "print('The accuracy :',accuracy_score(y_pred,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy : 0.7535014005602241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfCN4kRhe_Cm",
        "colab_type": "code",
        "outputId": "c39f5bd7-f454-4f81-9879-59dd705197d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# Calculate precision, recall, and fbeta_score\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.92      0.82       216\n",
            "           1       0.80      0.50      0.61       141\n",
            "\n",
            "    accuracy                           0.75       357\n",
            "   macro avg       0.77      0.71      0.72       357\n",
            "weighted avg       0.76      0.75      0.74       357\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqFnWhR3fFUy",
        "colab_type": "text"
      },
      "source": [
        "A larger dataset would result in a higher level of accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xtL5jQwgrvT",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Challenging the solution "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64BOFq4liUXu",
        "colab_type": "text"
      },
      "source": [
        "### <font color = cyan> Cleaning the Test Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maHca145ifH-",
        "colab_type": "code",
        "outputId": "f01fad79-16a8-493c-d57d-994931d6baf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# checking data types\n",
        "test.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      int64\n",
              "Pclass           int64\n",
              "Name            object\n",
              "Sex             object\n",
              "Age            float64\n",
              "SibSp            int64\n",
              "Parch            int64\n",
              "Ticket          object\n",
              "Fare           float64\n",
              "Cabin           object\n",
              "Embarked        object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 599
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-8VKRvMi5he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert the relevant data types to categorical\n",
        "\n",
        "# Creating a list of the numerical columns in the dataset.\n",
        "numeric = ['Age', 'Fare', 'SibSp', 'Parch']\n",
        "\n",
        "categoricals = ['Pclass','Sex','Embarked']\n",
        "\n",
        "objects = ['Name']\n",
        "\n",
        "for x in test.columns:\n",
        "   if x in numeric:\n",
        "       test[x]=pd.to_numeric(test[x])\n",
        "   elif x in categoricals:\n",
        "        test[x]=test[x].astype('category')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXBEp9EXi5cS",
        "colab_type": "code",
        "outputId": "ae17b3b1-2673-490e-e7d6-2da547cd46b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "test.dtypes"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId       int64\n",
              "Pclass         category\n",
              "Name             object\n",
              "Sex            category\n",
              "Age             float64\n",
              "SibSp             int64\n",
              "Parch             int64\n",
              "Ticket           object\n",
              "Fare            float64\n",
              "Cabin            object\n",
              "Embarked       category\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 601
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbQ8fVIij1d-",
        "colab_type": "code",
        "outputId": "570510b4-7756-4cab-9f4c-8f22eff3b2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "test.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      0\n",
              "Pclass           0\n",
              "Name             0\n",
              "Sex              0\n",
              "Age             86\n",
              "SibSp            0\n",
              "Parch            0\n",
              "Ticket           0\n",
              "Fare             1\n",
              "Cabin          327\n",
              "Embarked         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 602
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK5lU7fqi5as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# replacing all missing values with the mean in the age column\n",
        "test['Age'].fillna((test['Age'].mean()), inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L83AFtYwwoF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# column cabin containing categorical values \n",
        "# dropping the columns since it contains very many missing values\n",
        "test.drop('Cabin', axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz1rmFUJwaL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replace the null values in the Embarked column with the mode\n",
        "\n",
        "test['Fare'].fillna(test['Fare'].mode()[0], inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE2bamMkwxso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropping the unnecessary columns\n",
        "test.drop(['PassengerId', 'Ticket'], axis = 1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qvkohYP0kYA",
        "colab_type": "code",
        "outputId": "064b845d-106d-4676-8495-8f436e280916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "test.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pclass      0\n",
              "Name        0\n",
              "Sex         0\n",
              "Age         0\n",
              "SibSp       0\n",
              "Parch       0\n",
              "Fare        0\n",
              "Embarked    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 607
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnhVnlzmywH5",
        "colab_type": "text"
      },
      "source": [
        "### <font color = cyan> Feature Engineering for Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2hPdaIRyvP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's establish the title of each passenger\n",
        "\n",
        "for x in test:\n",
        "    test['Title'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auy_VcjQzEzj",
        "colab_type": "code",
        "outputId": "c3a30104-8570-4a79-9ebc-78971ca109f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#What are the titles extracted from the names ?\n",
        "test.Title.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Mr', 'Mrs', 'Miss', 'Master', 'Ms', 'Col', 'Rev', 'Dr', 'Dona'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 609
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xZXPPDEzJ1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in test:\n",
        "    test['Title'] = test['Title'].replace(['Col', 'Dr', 'Rev', 'Sir', 'Dona'], 'Other')\n",
        "\n",
        "    test['Title'] = test['Title'].replace('Mlle', 'Miss')\n",
        "    test['Title'] = test['Title'].replace('Ms', 'Miss')\n",
        "    test['Title'] = test['Title'].replace('Mme', 'Mrs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFnA5oXVzNHy",
        "colab_type": "code",
        "outputId": "bc252a8d-00a7-4e4e-b854-28ce89260978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#What are the titles extracted from the names?\n",
        "\n",
        "test.Title.unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Mr', 'Mrs', 'Miss', 'Master', 'Other'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 611
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_C4K3RgzRuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dropping the name column\n",
        "test.drop('Name', axis=1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93R0VyLlzSmC",
        "colab_type": "code",
        "outputId": "d7b30da5-42d9-4f2d-ad2c-aec8f73ae13b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "test.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Embarked</th>\n",
              "      <th>Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>Q</td>\n",
              "      <td>Mr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>S</td>\n",
              "      <td>Mrs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Pclass     Sex   Age  SibSp  Parch    Fare Embarked Title\n",
              "0      3    male  34.5      0      0  7.8292        Q    Mr\n",
              "1      3  female  47.0      1      0  7.0000        S   Mrs"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 613
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwXejXkqzZVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in test:\n",
        "    test['FamSize'] = test['SibSp'] + test['Parch'] + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFQ2whqczih0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in test:\n",
        "    test['Alone?'] = 0\n",
        "    test.loc[test['FamSize'] == 1, 'Alone?'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmR9pDCSzifO",
        "colab_type": "code",
        "outputId": "6c9ddcfc-9937-4f8f-cb2f-a973f5e3ab14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test['Alone?'].unique()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 616
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3yqFyjezZS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = test.drop(['Parch', 'SibSp', 'FamSize'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTL9RoJqx5St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categoricals = ['Sex','Embarked','Title']\n",
        "\n",
        "test = pd.get_dummies(test, columns=['Sex','Embarked','Title'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csnl8VKEyZ0W",
        "colab_type": "code",
        "outputId": "5577a3c5-10e6-45d2-d3f4-a5ac28a5e3bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "test.head(4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Age</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Alone?</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Title_Master</th>\n",
              "      <th>Title_Miss</th>\n",
              "      <th>Title_Mr</th>\n",
              "      <th>Title_Mrs</th>\n",
              "      <th>Title_Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>34.5</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>47.0</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>62.0</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>27.0</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Pclass   Age    Fare  Alone?  ...  Title_Miss  Title_Mr  Title_Mrs  Title_Other\n",
              "0      3  34.5  7.8292       1  ...           0         1          0            0\n",
              "1      3  47.0  7.0000       0  ...           0         0          1            0\n",
              "2      2  62.0  9.6875       1  ...           0         1          0            0\n",
              "3      3  27.0  8.6625       1  ...           0         1          0            0\n",
              "\n",
              "[4 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 620
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1VTzapYyTKn",
        "colab_type": "code",
        "outputId": "143d48b8-4186-4dd8-c070-d724a72368b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "test[\"Adult\"] = 0\n",
        "\n",
        "test[\"Adult\"][test[\"Age\"] >= 18] = 1\n",
        "\n",
        "#Thereafter drop the age column\n",
        "\n",
        "test = test.drop(['Age'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr5lfitl02w3",
        "colab_type": "code",
        "outputId": "8d94d484-d276-4d3c-fb73-271d943b847b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "test.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Pclass', 'Fare', 'Alone?', 'Sex_female', 'Sex_male', 'Embarked_C',\n",
              "       'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Miss', 'Title_Mr',\n",
              "       'Title_Mrs', 'Title_Other', 'Adult'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 624
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBIVoek50Lv7",
        "colab_type": "code",
        "outputId": "fa277625-2474-40e0-de3e-d41d52f00d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "test = test[['Pclass','Adult','Fare','Alone?','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S','Title_Master','Title_Miss','Title_Mr','Title_Mrs','Title_Other']]\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Adult</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Alone?</th>\n",
              "      <th>Sex_female</th>\n",
              "      <th>Sex_male</th>\n",
              "      <th>Embarked_C</th>\n",
              "      <th>Embarked_Q</th>\n",
              "      <th>Embarked_S</th>\n",
              "      <th>Title_Master</th>\n",
              "      <th>Title_Miss</th>\n",
              "      <th>Title_Mr</th>\n",
              "      <th>Title_Mrs</th>\n",
              "      <th>Title_Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Pclass  Adult     Fare  Alone?  ...  Title_Miss  Title_Mr  Title_Mrs  Title_Other\n",
              "0      3      1   7.8292       1  ...           0         1          0            0\n",
              "1      3      1   7.0000       0  ...           0         0          1            0\n",
              "2      2      1   9.6875       1  ...           0         1          0            0\n",
              "3      3      1   8.6625       1  ...           0         1          0            0\n",
              "4      3      1  12.2875       0  ...           0         0          1            0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 625
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioyX7sg-g60C",
        "colab_type": "text"
      },
      "source": [
        "### <font color = cyan> Random Forest Classifier\n",
        "\n",
        "* Using Random Forest which is an Ensemble algorithm.\n",
        "* Also considering the Train data without splitting it further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW8x0ZpAw9z0",
        "colab_type": "code",
        "outputId": "75cbb700-531a-487a-f2ad-48b63508d9f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "# Splitting the data into test and train sets as provided earlier\n",
        "# NOTE: No further split is done.\n",
        "\n",
        "X_train = data.iloc[:, :-1]\n",
        "y_train = data.iloc[:,-1]\n",
        "X_test = test.copy()\n",
        "\n",
        "# Instantiating our model\n",
        "# Training the model\n",
        "random_forest = RandomForestClassifier(criterion = \"gini\", \n",
        "                                       min_samples_leaf = 1, \n",
        "                                       min_samples_split = 10,   \n",
        "                                       n_estimators=100, \n",
        "                                       max_features='auto', \n",
        "                                       oob_score=True, \n",
        "                                       random_state=10,\n",
        "                                       n_jobs=-1) #  Whether to use out-of-bag samples to estimate the generalization accuracy.\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions\n",
        "y_prediction = random_forest.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "random_forest.score(X_train, y_train)\n",
        "\n",
        "acc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n",
        "print(round(acc_random_forest,2,), \"%\")\n",
        "\n",
        "# Generating cross-validated estimates for each input data point\n",
        "predictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\n",
        "\n",
        "\n",
        "print(confusion_matrix(y_train, predictions))\n",
        "print(classification_report(y_train, predictions))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88.89 %\n",
            "[[479  70]\n",
            " [ 94 248]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.87      0.85       549\n",
            "           1       0.78      0.73      0.75       342\n",
            "\n",
            "    accuracy                           0.82       891\n",
            "   macro avg       0.81      0.80      0.80       891\n",
            "weighted avg       0.81      0.82      0.81       891\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh2K0Eh53OPd",
        "colab_type": "text"
      },
      "source": [
        "* The Random Forest Classifier model yields 89% accuracy.\n",
        "\n",
        "* This is a better model compared to KNN.\n",
        "* However, tunning may be needed to assess the model adequately.\n",
        "\n",
        "* The Recall score increased compared to other models.\n",
        "\n",
        "* Interprating the confusion matrix:\n",
        "* The first row is about the not-survived-predictions:\n",
        "     * 479 passengers were correctly classified as not survived (called true negatives) \n",
        "     * 70 were wrongly classified as not survived (false positives).\n",
        "* The second row is about the survived-predictions: \n",
        "     * 94 passengers where wrongly classified as survived (false negatives) and\n",
        "     * 248 were correctly classified as survived (true positives).\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEkyk3vS6VAj",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9X772SY6VsJ",
        "colab_type": "text"
      },
      "source": [
        "**From the analysis: Tuning and Optimizing the KNN model is highly recommended to improve the performance of the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34wxiA8Y1VO3",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4N1ACok1RdN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* From the analysis: Tuning and Optimizing the KNN model is highly recommended to improve the performance of the model.\n",
        "* Splitting the data into 70,30 train and test sets was the best in the Titanic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epuo2NJP7ovp",
        "colab_type": "text"
      },
      "source": [
        "# <font color = yellow> **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfz9sECMfliD",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKjeGeCafgKD",
        "colab_type": "code",
        "outputId": "2fe97749-02e1-405c-f9fb-74cab81bb110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "spam = pd.read_csv('spambase.data')\n",
        "spam"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>0.64</th>\n",
              "      <th>0.64.1</th>\n",
              "      <th>0.1</th>\n",
              "      <th>0.32</th>\n",
              "      <th>0.2</th>\n",
              "      <th>0.3</th>\n",
              "      <th>0.4</th>\n",
              "      <th>0.5</th>\n",
              "      <th>0.6</th>\n",
              "      <th>0.7</th>\n",
              "      <th>0.64.2</th>\n",
              "      <th>0.8</th>\n",
              "      <th>0.9</th>\n",
              "      <th>0.10</th>\n",
              "      <th>0.32.1</th>\n",
              "      <th>0.11</th>\n",
              "      <th>1.29</th>\n",
              "      <th>1.93</th>\n",
              "      <th>0.12</th>\n",
              "      <th>0.96</th>\n",
              "      <th>0.13</th>\n",
              "      <th>0.14</th>\n",
              "      <th>0.15</th>\n",
              "      <th>0.16</th>\n",
              "      <th>0.17</th>\n",
              "      <th>0.18</th>\n",
              "      <th>0.19</th>\n",
              "      <th>0.20</th>\n",
              "      <th>0.21</th>\n",
              "      <th>0.22</th>\n",
              "      <th>0.23</th>\n",
              "      <th>0.24</th>\n",
              "      <th>0.25</th>\n",
              "      <th>0.26</th>\n",
              "      <th>0.27</th>\n",
              "      <th>0.28</th>\n",
              "      <th>0.29</th>\n",
              "      <th>0.30</th>\n",
              "      <th>0.31</th>\n",
              "      <th>0.32.2</th>\n",
              "      <th>0.33</th>\n",
              "      <th>0.34</th>\n",
              "      <th>0.35</th>\n",
              "      <th>0.36</th>\n",
              "      <th>0.37</th>\n",
              "      <th>0.38</th>\n",
              "      <th>0.39</th>\n",
              "      <th>0.40</th>\n",
              "      <th>0.41</th>\n",
              "      <th>0.42</th>\n",
              "      <th>0.778</th>\n",
              "      <th>0.43</th>\n",
              "      <th>0.44</th>\n",
              "      <th>3.756</th>\n",
              "      <th>61</th>\n",
              "      <th>278</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4595</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4600 rows × 58 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0  0.64  0.64.1  0.1  0.32   0.2  ...   0.43   0.44  3.756   61   278  1\n",
              "0     0.21  0.28    0.50  0.0  0.14  0.28  ...  0.180  0.048  5.114  101  1028  1\n",
              "1     0.06  0.00    0.71  0.0  1.23  0.19  ...  0.184  0.010  9.821  485  2259  1\n",
              "2     0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "3     0.00  0.00    0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191  1\n",
              "4     0.00  0.00    0.00  0.0  1.85  0.00  ...  0.000  0.000  3.000   15    54  1\n",
              "...    ...   ...     ...  ...   ...   ...  ...    ...    ...    ...  ...   ... ..\n",
              "4595  0.31  0.00    0.62  0.0  0.00  0.31  ...  0.000  0.000  1.142    3    88  0\n",
              "4596  0.00  0.00    0.00  0.0  0.00  0.00  ...  0.000  0.000  1.555    4    14  0\n",
              "4597  0.30  0.00    0.30  0.0  0.00  0.00  ...  0.000  0.000  1.404    6   118  0\n",
              "4598  0.96  0.00    0.00  0.0  0.32  0.00  ...  0.000  0.000  1.147    5    78  0\n",
              "4599  0.00  0.00    0.65  0.0  0.00  0.00  ...  0.000  0.000  1.250    5    40  0\n",
              "\n",
              "[4600 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H3iOTjobifO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the column names\n",
        "spam.columns = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d', 'word_freq_our',\n",
        "'word_freq_over', 'word_freq_remove', 'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
        "'word_freq_receive','word_freq_will','word_freq_people','word_freq_report','word_freq_addresses',\n",
        "'word_freq_free','word_freq_business','word_freq_email','word_freq_you','word_freq_credit','word_freq_your',\n",
        "'word_freq_font','word_freq_000','word_freq_money','word_freq_hp','word_freq_hpl','word_freq_george',\n",
        "'word_freq_650','word_freq_lab','word_freq_labs','word_freq_telnet','word_freq_857','word_freq_data',\n",
        "'word_freq_415','word_freq_85','word_freq_technology','word_freq_1999','word_freq_parts','word_freq_pm',\n",
        "'word_freq_direct','word_freq_cs','word_freq_meeting','word_freq_original','word_freq_project',\n",
        "'word_freq_re','word_freq_edu','word_freq_table','word_freq_conference','char_freq_;','char_freq_(',\n",
        "'char_freq_[','char_freq_exclamation','char_freq_dollar','char_freq_hashtag','capital_run_length_average',\n",
        "'capital_run_length_longest','capital_run_length_total','spam']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ-q3BwUf10x",
        "colab_type": "code",
        "outputId": "463de534-a4ce-4560-a851-0da371ec2b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "# previewing the top of the dataset\n",
        "spam.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_exclamation</th>\n",
              "      <th>char_freq_dollar</th>\n",
              "      <th>char_freq_hashtag</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0            0.21               0.28  ...                      1028     1\n",
              "1            0.06               0.00  ...                      2259     1\n",
              "2            0.00               0.00  ...                       191     1\n",
              "3            0.00               0.00  ...                       191     1\n",
              "4            0.00               0.00  ...                        54     1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmVkQn2s5afS",
        "colab_type": "code",
        "outputId": "1bf56814-9b8e-4f3d-cc3c-cf03cc2d4972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "#previewing the bottom of the dataset\n",
        "spam.tail()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_exclamation</th>\n",
              "      <th>char_freq_dollar</th>\n",
              "      <th>char_freq_hashtag</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4595</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "4595            0.31                0.0  ...                        88     0\n",
              "4596            0.00                0.0  ...                        14     0\n",
              "4597            0.30                0.0  ...                       118     0\n",
              "4598            0.96                0.0  ...                        78     0\n",
              "4599            0.00                0.0  ...                        40     0\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QgkSkJg6Gsx",
        "colab_type": "code",
        "outputId": "946d58c0-a6f6-4f79-ee7b-a2b5005f9d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "# checking the summary statistics of our dataset\n",
        "spam.describe()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_exclamation</th>\n",
              "      <th>char_freq_dollar</th>\n",
              "      <th>char_freq_hashtag</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "      <td>4600.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104576</td>\n",
              "      <td>0.212922</td>\n",
              "      <td>0.280578</td>\n",
              "      <td>0.065439</td>\n",
              "      <td>0.312222</td>\n",
              "      <td>0.095922</td>\n",
              "      <td>0.114233</td>\n",
              "      <td>0.105317</td>\n",
              "      <td>0.090087</td>\n",
              "      <td>0.239465</td>\n",
              "      <td>0.059837</td>\n",
              "      <td>0.541680</td>\n",
              "      <td>0.093950</td>\n",
              "      <td>0.058639</td>\n",
              "      <td>0.049215</td>\n",
              "      <td>0.248833</td>\n",
              "      <td>0.142617</td>\n",
              "      <td>0.184504</td>\n",
              "      <td>1.662041</td>\n",
              "      <td>0.085596</td>\n",
              "      <td>0.809728</td>\n",
              "      <td>0.121228</td>\n",
              "      <td>0.101667</td>\n",
              "      <td>0.094289</td>\n",
              "      <td>0.549624</td>\n",
              "      <td>0.265441</td>\n",
              "      <td>0.767472</td>\n",
              "      <td>0.124872</td>\n",
              "      <td>0.098937</td>\n",
              "      <td>0.102874</td>\n",
              "      <td>0.064767</td>\n",
              "      <td>0.047059</td>\n",
              "      <td>0.097250</td>\n",
              "      <td>0.047846</td>\n",
              "      <td>0.105435</td>\n",
              "      <td>0.097498</td>\n",
              "      <td>0.136983</td>\n",
              "      <td>0.013204</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>0.064848</td>\n",
              "      <td>0.043676</td>\n",
              "      <td>0.132367</td>\n",
              "      <td>0.046109</td>\n",
              "      <td>0.079213</td>\n",
              "      <td>0.301289</td>\n",
              "      <td>0.179863</td>\n",
              "      <td>0.005446</td>\n",
              "      <td>0.031876</td>\n",
              "      <td>0.038583</td>\n",
              "      <td>0.139061</td>\n",
              "      <td>0.016980</td>\n",
              "      <td>0.268960</td>\n",
              "      <td>0.075827</td>\n",
              "      <td>0.044248</td>\n",
              "      <td>5.191827</td>\n",
              "      <td>52.170870</td>\n",
              "      <td>283.290435</td>\n",
              "      <td>0.393913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305387</td>\n",
              "      <td>1.290700</td>\n",
              "      <td>0.504170</td>\n",
              "      <td>1.395303</td>\n",
              "      <td>0.672586</td>\n",
              "      <td>0.273850</td>\n",
              "      <td>0.391480</td>\n",
              "      <td>0.401112</td>\n",
              "      <td>0.278643</td>\n",
              "      <td>0.644816</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.861791</td>\n",
              "      <td>0.301065</td>\n",
              "      <td>0.335219</td>\n",
              "      <td>0.258871</td>\n",
              "      <td>0.825881</td>\n",
              "      <td>0.444099</td>\n",
              "      <td>0.530930</td>\n",
              "      <td>1.775669</td>\n",
              "      <td>0.509821</td>\n",
              "      <td>1.200938</td>\n",
              "      <td>1.025866</td>\n",
              "      <td>0.350321</td>\n",
              "      <td>0.442681</td>\n",
              "      <td>1.671511</td>\n",
              "      <td>0.887043</td>\n",
              "      <td>3.367639</td>\n",
              "      <td>0.538631</td>\n",
              "      <td>0.593389</td>\n",
              "      <td>0.456729</td>\n",
              "      <td>0.403435</td>\n",
              "      <td>0.328594</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.329480</td>\n",
              "      <td>0.532315</td>\n",
              "      <td>0.402664</td>\n",
              "      <td>0.423493</td>\n",
              "      <td>0.220675</td>\n",
              "      <td>0.434718</td>\n",
              "      <td>0.349953</td>\n",
              "      <td>0.361243</td>\n",
              "      <td>0.766900</td>\n",
              "      <td>0.223835</td>\n",
              "      <td>0.622042</td>\n",
              "      <td>1.011787</td>\n",
              "      <td>0.911214</td>\n",
              "      <td>0.076283</td>\n",
              "      <td>0.285765</td>\n",
              "      <td>0.243497</td>\n",
              "      <td>0.270377</td>\n",
              "      <td>0.109406</td>\n",
              "      <td>0.815726</td>\n",
              "      <td>0.245906</td>\n",
              "      <td>0.429388</td>\n",
              "      <td>31.732891</td>\n",
              "      <td>194.912453</td>\n",
              "      <td>606.413764</td>\n",
              "      <td>0.488669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.275500</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.314250</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.705250</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>265.250000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       word_freq_make  word_freq_address  ...  capital_run_length_total         spam\n",
              "count     4600.000000        4600.000000  ...               4600.000000  4600.000000\n",
              "mean         0.104576           0.212922  ...                283.290435     0.393913\n",
              "std          0.305387           1.290700  ...                606.413764     0.488669\n",
              "min          0.000000           0.000000  ...                  1.000000     0.000000\n",
              "25%          0.000000           0.000000  ...                 35.000000     0.000000\n",
              "50%          0.000000           0.000000  ...                 95.000000     0.000000\n",
              "75%          0.000000           0.000000  ...                265.250000     1.000000\n",
              "max          4.540000          14.280000  ...              15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPLltUoN6bJf",
        "colab_type": "text"
      },
      "source": [
        "The dataset has too many columns to analyse manually.\n",
        "we really need reduction methods here to find the most important features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qXbIh956Ppl",
        "colab_type": "code",
        "outputId": "6eb40862-1cda-4e47-e7fc-9fe734f6cee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# checking summary information about our dataset\n",
        "spam.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4600 entries, 0 to 4599\n",
            "Data columns (total 58 columns):\n",
            "word_freq_make                4600 non-null float64\n",
            "word_freq_address             4600 non-null float64\n",
            "word_freq_all                 4600 non-null float64\n",
            "word_freq_3d                  4600 non-null float64\n",
            "word_freq_our                 4600 non-null float64\n",
            "word_freq_over                4600 non-null float64\n",
            "word_freq_remove              4600 non-null float64\n",
            "word_freq_internet            4600 non-null float64\n",
            "word_freq_order               4600 non-null float64\n",
            "word_freq_mail                4600 non-null float64\n",
            "word_freq_receive             4600 non-null float64\n",
            "word_freq_will                4600 non-null float64\n",
            "word_freq_people              4600 non-null float64\n",
            "word_freq_report              4600 non-null float64\n",
            "word_freq_addresses           4600 non-null float64\n",
            "word_freq_free                4600 non-null float64\n",
            "word_freq_business            4600 non-null float64\n",
            "word_freq_email               4600 non-null float64\n",
            "word_freq_you                 4600 non-null float64\n",
            "word_freq_credit              4600 non-null float64\n",
            "word_freq_your                4600 non-null float64\n",
            "word_freq_font                4600 non-null float64\n",
            "word_freq_000                 4600 non-null float64\n",
            "word_freq_money               4600 non-null float64\n",
            "word_freq_hp                  4600 non-null float64\n",
            "word_freq_hpl                 4600 non-null float64\n",
            "word_freq_george              4600 non-null float64\n",
            "word_freq_650                 4600 non-null float64\n",
            "word_freq_lab                 4600 non-null float64\n",
            "word_freq_labs                4600 non-null float64\n",
            "word_freq_telnet              4600 non-null float64\n",
            "word_freq_857                 4600 non-null float64\n",
            "word_freq_data                4600 non-null float64\n",
            "word_freq_415                 4600 non-null float64\n",
            "word_freq_85                  4600 non-null float64\n",
            "word_freq_technology          4600 non-null float64\n",
            "word_freq_1999                4600 non-null float64\n",
            "word_freq_parts               4600 non-null float64\n",
            "word_freq_pm                  4600 non-null float64\n",
            "word_freq_direct              4600 non-null float64\n",
            "word_freq_cs                  4600 non-null float64\n",
            "word_freq_meeting             4600 non-null float64\n",
            "word_freq_original            4600 non-null float64\n",
            "word_freq_project             4600 non-null float64\n",
            "word_freq_re                  4600 non-null float64\n",
            "word_freq_edu                 4600 non-null float64\n",
            "word_freq_table               4600 non-null float64\n",
            "word_freq_conference          4600 non-null float64\n",
            "char_freq_;                   4600 non-null float64\n",
            "char_freq_(                   4600 non-null float64\n",
            "char_freq_[                   4600 non-null float64\n",
            "char_freq_exclamation         4600 non-null float64\n",
            "char_freq_dollar              4600 non-null float64\n",
            "char_freq_hashtag             4600 non-null float64\n",
            "capital_run_length_average    4600 non-null float64\n",
            "capital_run_length_longest    4600 non-null int64\n",
            "capital_run_length_total      4600 non-null int64\n",
            "spam                          4600 non-null int64\n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMC9cTYn6r7_",
        "colab_type": "code",
        "outputId": "c296acc3-ea6d-4688-83ad-3cf447b41677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spam.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4600, 58)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FJjXbgW6yho",
        "colab_type": "code",
        "outputId": "7c3ad50f-1b87-48ba-c13f-6c0273fb1fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "spam.isnull().sum()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                0\n",
              "word_freq_address             0\n",
              "word_freq_all                 0\n",
              "word_freq_3d                  0\n",
              "word_freq_our                 0\n",
              "word_freq_over                0\n",
              "word_freq_remove              0\n",
              "word_freq_internet            0\n",
              "word_freq_order               0\n",
              "word_freq_mail                0\n",
              "word_freq_receive             0\n",
              "word_freq_will                0\n",
              "word_freq_people              0\n",
              "word_freq_report              0\n",
              "word_freq_addresses           0\n",
              "word_freq_free                0\n",
              "word_freq_business            0\n",
              "word_freq_email               0\n",
              "word_freq_you                 0\n",
              "word_freq_credit              0\n",
              "word_freq_your                0\n",
              "word_freq_font                0\n",
              "word_freq_000                 0\n",
              "word_freq_money               0\n",
              "word_freq_hp                  0\n",
              "word_freq_hpl                 0\n",
              "word_freq_george              0\n",
              "word_freq_650                 0\n",
              "word_freq_lab                 0\n",
              "word_freq_labs                0\n",
              "word_freq_telnet              0\n",
              "word_freq_857                 0\n",
              "word_freq_data                0\n",
              "word_freq_415                 0\n",
              "word_freq_85                  0\n",
              "word_freq_technology          0\n",
              "word_freq_1999                0\n",
              "word_freq_parts               0\n",
              "word_freq_pm                  0\n",
              "word_freq_direct              0\n",
              "word_freq_cs                  0\n",
              "word_freq_meeting             0\n",
              "word_freq_original            0\n",
              "word_freq_project             0\n",
              "word_freq_re                  0\n",
              "word_freq_edu                 0\n",
              "word_freq_table               0\n",
              "word_freq_conference          0\n",
              "char_freq_;                   0\n",
              "char_freq_(                   0\n",
              "char_freq_[                   0\n",
              "char_freq_exclamation         0\n",
              "char_freq_dollar              0\n",
              "char_freq_hashtag             0\n",
              "capital_run_length_average    0\n",
              "capital_run_length_longest    0\n",
              "capital_run_length_total      0\n",
              "spam                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-fn2-yr6mUD",
        "colab_type": "text"
      },
      "source": [
        "* The data has 4600 rows and 58 columns\n",
        "* We have no missing values\n",
        "* The columns are too many and we need to used reduction methods to reduce the data dimension.\n",
        "* We will go straight to modelling since even making charts can't help in analysing the columns.\n",
        "* We will plot the target variable though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC1tDjz26nby",
        "colab_type": "code",
        "outputId": "78d55425-942c-46c5-e6c7-a2d056c99103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "source": [
        "# Plotting the target variable \n",
        "# using seaborn\n",
        "\n",
        "sns.countplot(spam.spam)\n",
        "plt.title('Spam vs Non_Spam Emails')\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlZNMJgAfSx0",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Naives Bayes Classifier\n",
        "Perform classification of the testing set samples using the Naive Bayes Classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEAj4sEZ7iBB",
        "colab_type": "text"
      },
      "source": [
        "* The Naive Bayes Classifier is a statistical classification technique based on the Bayes Theorem. \n",
        "\n",
        "* It has high accuracy and speed on large datasets.\n",
        "\n",
        "* This type of classifier takes into account the assumption that the effect of a particular feature in a class is independent of other features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVg0XobB7FOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create correlation matrix\n",
        "corr_matrix = spam.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
        "\n",
        "# Drop the highly correlated features \n",
        "spam.drop(spam[to_drop], axis=1, inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnYxdU8N7_F-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8da4793-2134-4d1e-b2eb-401f2aef0791"
      },
      "source": [
        "# checking if there are any correlated features\n",
        "to_drop"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['word_freq_415']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8S-gVZP8M26",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "No columns were dropped since there are no correlated features. we will use PCA and LDA to reduce the data dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH-ZNorz8QkO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Splitting the Spam data into 80, 20 train and test sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpV_A6NA7_D1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "bf16f1be-fd48-4b6d-88d6-9db67e9b6b85"
      },
      "source": [
        "# Fitting the Naives Bayes Classifier: GausssianNB since the features are continuous\n",
        "# Splitting the data\n",
        "\n",
        "X = spam.iloc[:, 0:-1]\n",
        "y = spam.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "model = gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9032608695652173\n",
            "\n",
            "\n",
            "[[513  25]\n",
            " [ 64 318]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.95      0.92       538\n",
            "           1       0.93      0.83      0.88       382\n",
            "\n",
            "    accuracy                           0.90       920\n",
            "   macro avg       0.91      0.89      0.90       920\n",
            "weighted avg       0.90      0.90      0.90       920\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOWzZCFa8jRH",
        "colab_type": "text"
      },
      "source": [
        "* The 80,20 split model yielded 90.3% accuracy.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 513 emails were correctly classified as Ham (called true negatives) \n",
        "     * 25 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 64 emails where wrongly classified as spam (false negatives) and\n",
        "     * 318 were correctly classified as Spam (true positives).\n",
        "     \n",
        "* In this classification we value the Recall so much. \n",
        "* The Recall is 95% which means that this is good model.\n",
        "* Recall is the ability of a model to find all the relevant cases within a dataset.\n",
        "* It is also called sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1nxr_-e7-_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "1cb363b8-d20c-4ebf-dc72-faf477b637c4"
      },
      "source": [
        "# Fitting the Naives Bayes Classifier: GausssianNB since the features are continuous\n",
        "# Splitting the data\n",
        "\n",
        "X = spam.iloc[:, 0:-1]\n",
        "y = spam.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "model = gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9036231884057971\n",
            "\n",
            "\n",
            "[[789  33]\n",
            " [100 458]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.96      0.92       822\n",
            "           1       0.93      0.82      0.87       558\n",
            "\n",
            "    accuracy                           0.90      1380\n",
            "   macro avg       0.91      0.89      0.90      1380\n",
            "weighted avg       0.91      0.90      0.90      1380\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDVUhYYp8wBZ",
        "colab_type": "text"
      },
      "source": [
        "* The 70,30 split model also yielded 90.3% accuracy.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 789 emails were correctly classified as Ham (called true negatives) \n",
        "     * 33 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 100 emails where wrongly classified as spam (false negatives) and\n",
        "     * 458 were correctly classified as Spam (true positives).\n",
        "     \n",
        "* The recall improved slightly from 95% in the previous model to 96%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yq8zRIg7-9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d1eb232e-abc9-44b2-86af-ad6fead30b5b"
      },
      "source": [
        "# Fitting the Naives Bayes Classifier: GausssianNB since the features are continuous\n",
        "# Splitting the data\n",
        "\n",
        "X = spam.iloc[:, 0:-1]\n",
        "y = spam.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "model = gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9081521739130435\n",
            "\n",
            "\n",
            "[[1060   37]\n",
            " [ 132  611]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93      1097\n",
            "           1       0.94      0.82      0.88       743\n",
            "\n",
            "    accuracy                           0.91      1840\n",
            "   macro avg       0.92      0.89      0.90      1840\n",
            "weighted avg       0.91      0.91      0.91      1840\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHiyN4k9872D",
        "colab_type": "text"
      },
      "source": [
        "* The 60,40 split model yielded 90.8% accuracy\n",
        "* This is a slight improvement compared to the previous split models.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 1060 emails were correctly classified as Ham (called true negatives) \n",
        "     * 37 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 132 emails where wrongly classified as spam (false negatives) and\n",
        "     * 611 were correctly classified as Spam (true positives)..\n",
        "     \n",
        "* Again here the recall increased to 97%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucen1QB69HIR",
        "colab_type": "text"
      },
      "source": [
        "### <font color = cyan> Optimizing Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EiygfLk9EKu",
        "colab_type": "text"
      },
      "source": [
        "Improving the performance of the Naive Bayes classifier:\n",
        " * Normalizing our data\n",
        " * Remove Redundant/ correlated features\n",
        " * Apply smoothing techniques; If our dataset has zero frequency issue, we can apply smoothing techniques such as \"Laplace Correction\" to predict the class of the test data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkSosI319Y7w",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We have already applied the first techniques in our models which was relevant in this scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ngDLFwO-Yd9",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Challenging the Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrlsUrEn-jxR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "3f7be4f6-b2dd-4500-899c-2e9e7d072098"
      },
      "source": [
        "# Fitting the Support Vector Classifier\n",
        "# Splitting the data\n",
        "\n",
        "X = spam.iloc[:, 0:-1]\n",
        "y = spam.iloc[:,-1]\n",
        "\n",
        "# transform = Normalizer()\n",
        "# X = transform.transform(X)\n",
        "\n",
        "# X = normalize(X, norm = 'l2')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state = 0)\n",
        "\n",
        "sc = StandardScaler(with_std = False, with_mean = False)\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "\n",
        "# pca = PCA(n_components = 1)\n",
        "# X_train = pca.fit_transform(X_train)\n",
        "# X_test = pca.transform(X_test)\n",
        "\n",
        "lda = LDA(n_components=10)\n",
        "X_train = lda.fit_transform(X_train, y_train)\n",
        "X_test = lda.transform(X_test)\n",
        "\n",
        "svc = SVC(C=0.1, gamma=0.001, kernel = 'linear')\n",
        "\n",
        "model = svc.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Evaluating our model using accuracy score, confusion matrix and classification report.\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9228260869565217\n",
            "\n",
            "\n",
            "[[1043   54]\n",
            " [  88  655]]\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.95      0.94      1097\n",
            "           1       0.92      0.88      0.90       743\n",
            "\n",
            "    accuracy                           0.92      1840\n",
            "   macro avg       0.92      0.92      0.92      1840\n",
            "weighted avg       0.92      0.92      0.92      1840\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(56, 2 - 1) = 1 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ttpz1-TZ-tK6",
        "colab_type": "text"
      },
      "source": [
        "* The Support Vector Machine model yielded 92.2% accuracy.\n",
        "* This is a great improvement compared to the Naive Bayes Gaussian models.\n",
        "* Interpreting the confusion matrix;\n",
        "* The first row is about the non-spam-predictions:\n",
        "     * 10 emails were correctly classified as Ham (called true negatives) \n",
        "     * 54 were wrongly classified as ham (false positives).\n",
        "* The second row is about the spam-predictions: \n",
        "     * 88 emails where wrongly classified as spam (false negatives) and\n",
        "     * 655 were correctly classified as Spam (true positives)..\n",
        "     \n",
        "* Again here the recall increased to 95%.\n",
        "* This is slight reduction.\n",
        "* Though the accuracy improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RMigiz2fX3B",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Recommendation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EwDVw-o9bEn",
        "colab_type": "text"
      },
      "source": [
        "* Normalizing or standardizing the features works greatly to improve the classifier.\n",
        "* For the spam detection challenge project, using the Standard Scaler yielded the best results.\n",
        "* Also, using the linear discriminant analysis for dimension reduction yields beter results compared to PCA.\n",
        "* Lastly increasing the test size in the spam dataset imporved both the accuracy and the Recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEJEVFt2-7Wk",
        "colab_type": "text"
      },
      "source": [
        "## <font color = red> Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4FqRK_w-6N2",
        "colab_type": "text"
      },
      "source": [
        "* The Gaussian NB is the best model since it yielded both the best Accuracy and Recall scores.\n",
        "* Optimizing the model is very essential:\n",
        "    1. Scaling or normalizing the features\n",
        "    2. Reducing the data dimensions\n",
        "    3. Increasing the test size for a large dataset."
      ]
    }
  ]
}